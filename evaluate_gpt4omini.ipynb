{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc85de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (1.76.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\msi-gs66 stealth\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\msi-gs66 stealth\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\msi-gs66 stealth\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\msi-gs66 stealth\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\msi-gs66 stealth\\.conda\\envs\\ai_builders\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi-gs66 stealth\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#completeness\n",
    "import pandas as pd\n",
    "import time\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "df = pd.read_csv(\"eval_manual_check_dataset.csv\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an expert evaluator. Your task is to judge how completely a model's answer captures the key ideas of an ideal answer to a given question.\n",
    "\n",
    "You will be shown:\n",
    "- The original question\n",
    "- An ideal reference answer\n",
    "- A model-generated answer\n",
    "\n",
    "Your goal is to evaluate how well the model answer includes the core ideas of the ideal answer.\n",
    "\n",
    "Scoring Criteria:\n",
    "- 0 = The model's answer misses the point completely (none of the main ideas are present)\n",
    "- 1 = The model's answer includes some but not all of the main ideas\n",
    "- 2 = The model's answer includes all the main ideas clearly and accurately\n",
    "\n",
    "Respond in this exact format:\n",
    "Completeness: X\n",
    "\n",
    "Only respond with the score number. Do not add explanations or additional comments.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Ideal Answer:\n",
    "{ideal_answer}\n",
    "\n",
    "Model Answer:\n",
    "{model_answer}\n",
    "\"\"\"\n",
    "\n",
    "scores = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    filled_prompt = prompt_template.format(\n",
    "        question=row[\"Question\"],\n",
    "        ideal_answer=row[\"Ideal Answer\"],\n",
    "        model_answer=row[\"Model Answer\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator.\"},\n",
    "                {\"role\": \"user\", \"content\": filled_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "        if \"Completeness:\" in reply:\n",
    "            score = reply.split(\"Completeness:\")[1].strip().split()[0]\n",
    "        else:\n",
    "            score = \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        score = \"\"\n",
    "\n",
    "    scores.append(score)\n",
    "    time.sleep(1)\n",
    "\n",
    "df[\"completeness\"] = scores\n",
    "df.to_csv(\"eval_with_completeness.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e3b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebundancy_ratio\n",
    "import pandas as pd\n",
    "import time\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "df = pd.read_csv(\"eval_manual_check_dataset.csv\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an expert evaluator. Your task is to judge the Redundancy Ratio of a model-generated answer by comparing it to an ideal reference answer.\n",
    "\n",
    "Your goal is to determine whether the model included more information than what was expected based on the ideal answer.\n",
    "\n",
    "Scoring Criteria:\n",
    "- 0 = The model gives exactly the information expected â€” no more, no less.\n",
    "- 1 = The model adds a little more than what was expected, but it's acceptable.\n",
    "- 2 = The model adds significantly more information than desired.\n",
    "\n",
    "Respond in this exact format:\n",
    "Redundancy Ratio: X\n",
    "\n",
    "Only respond with the score number. Do not add explanations or comments.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Ideal Answer:\n",
    "{ideal_answer}\n",
    "\n",
    "Model Answer:\n",
    "{model_answer}\n",
    "\"\"\"\n",
    "\n",
    "scores = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    filled_prompt = prompt_template.format(\n",
    "        question=row[\"Question\"],\n",
    "        ideal_answer=row[\"Ideal Answer\"],\n",
    "        model_answer=row[\"Model Answer\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator.\"},\n",
    "                {\"role\": \"user\", \"content\": filled_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "        if \"Redundancy Ratio:\" in reply:\n",
    "            score = reply.split(\"Redundancy Ratio:\")[1].strip().split()[0]\n",
    "        else:\n",
    "            score = \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        score = \"\"\n",
    "\n",
    "    scores.append(score)\n",
    "    time.sleep(1)\n",
    "\n",
    "df[\"redundancy_ratio\"] = scores\n",
    "df.to_csv(\"eval_with_rebundancy_ratio_gpt4omini.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5235632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebundancy_severity\n",
    "import pandas as pd\n",
    "import time\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "df = pd.read_csv(\"eval_manual_check_dataset.csv\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an expert evaluator. Your task is to judge the Redundancy Severity of a model-generated answer by comparing it to an ideal reference answer.\n",
    "\n",
    "Your goal is to evaluate the **impact** of any extra or redundant content in the model's answer. Focus on how distracting, confusing, or misleading it is compared to the expected answer.\n",
    "\n",
    "Scoring Criteria:\n",
    "- 0 = No redundancy. The answer is focused and accurate.\n",
    "- 1 = Minor distraction. There is some unnecessary information, but it does not confuse the meaning.\n",
    "- 2 = Major misunderstanding. The extra content significantly distracts, misleads, or alters the expected meaning.\n",
    "\n",
    "Respond in this exact format:\n",
    "Redundancy Severity: X\n",
    "\n",
    "Only respond with the score number. Do not add explanations or comments.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Ideal Answer:\n",
    "{ideal_answer}\n",
    "\n",
    "Model Answer:\n",
    "{model_answer}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "scores = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    filled_prompt = prompt_template.format(\n",
    "        question=row[\"Question\"],\n",
    "        ideal_answer=row[\"Ideal Answer\"],\n",
    "        model_answer=row[\"Model Answer\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator.\"},\n",
    "                {\"role\": \"user\", \"content\": filled_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "        if \"Redundancy Severity:\" in reply:\n",
    "            score = reply.split(\"Redundancy Severity:\")[1].strip().split()[0]\n",
    "        else:\n",
    "            score = \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        score = \"\"\n",
    "\n",
    "    scores.append(score)\n",
    "    time.sleep(1)\n",
    "\n",
    "df[\"redundancy_severity\"] = scores\n",
    "df.to_csv(\"eval_with_rebundancy_severity_gpt4omini.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_builders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
